{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's new in Spark 3.0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Not much development in MLLIB library especially MLLIB with RDD interface.\n",
    "2. Spark 3 is significantly faster. Adaptive Execution and Dynamic Partitioning Pruning.\n",
    "3. Python 2 support is deprecated.\n",
    "4. Deeper Kubernetes support.\n",
    "5. Support for Binary files e.g. images, video etc.\n",
    "6. SparkGraph. Uses Cypher query language.\n",
    "7. ACID support in data lakes i.e. Delta Lakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD - Resilient, Distributed, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RDDs can be created using many ways. E.g. paraellize, sc.textFile, Hive, JDBC connectors, Cassandara, HBASe etc.\n",
    "2. Transformations: map, flatmap, filter, distinct, sample, union, intersection, substract, cartesian etc.\n",
    "3. Actions: collect, count, countByValue, take, top, reduce etc.\n",
    "4. Lazy Evaluation: Nothing happens until an action is being called.\n",
    "5. RDD was the primary user-facing API in Spark since its inception. At the core, an RDD is an immutable distributed collection of elements of your data, partitioned across nodes in your cluster that can be operated in parallel with a low-level API that offers transformations and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use RDDs?\n",
    "Consider these scenarios or common use cases for using RDDs when:\n",
    "\n",
    "1. you want low-level transformation and actions and control on your dataset;\n",
    "2. your data is unstructured, such as media streams or streams of text;\n",
    "3. you want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "4. you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column; and\n",
    "5. you can forgo some optimization and performance benefits available with DataFrames and Datasets for structured and semi-structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Created by Driver Program\n",
    "2. Is responsible for making RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Contains Row Objects\n",
    "2. Can Run SQL Queries\n",
    "3. Can have schema (leading to more effecient storage)\n",
    "4. Read and write to JSON, Parquet, Hive, csv, etc.\n",
    "5. Communicates with JDBC/ODBC, Tableau etc.\n",
    "6. Data Frames allow for better interoperability\n",
    "7. Data Frames simplify development e.g. you can perform most SQL operation with few lines of code.\n",
    "8. RDDs are good for map redcue kind of problems so sometimes you might need to convert a DF to an RDD e.g. dataframe.rdd().map(mapper_function)\n",
    "9. Like an RDD, a DataFrame is an immutable distributed collection of data. Unlike an RDD, data is organized into named columns, like a table in a relational database. Designed to make large data sets processing even easier, DataFrame allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction; it provides a domain specific language API to manipulate your distributed data; and makes Spark accessible to a wider audience, beyond specialized data engineers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Sets are more used in Scala then Python. Since Python and R have no compile-time type-safety, we only have untyped APIs, namely DataFrames.\n",
    "2. Data Sets are typed API and Data Frames are Untyped APIs.\n",
    "3. Conceptually, consider DataFrame as an alias for a collection of generic objects Dataset[Row], where a Row is a generic untyped JVM object. Dataset, by contrast, is a collection of strongly-typed JVM objects, dictated by a case class you define in Scala or a class in Java.\n",
    "4. Second, since Spark as a compiler understands your Dataset type JVM object, it maps your type-specific JVM object to Tungsten’s internal memory representation using Encoders. As a result, Tungsten Encoders can efficiently serialize/deserialize JVM objects as well as generate compact bytecode that can execute at superior speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should I use DataFrames or Datasets?\n",
    "1. If you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame or Dataset.\n",
    "2. If your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame or Dataset.\n",
    "3. If you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.\n",
    "4. If you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "5. If you are a R user, use DataFrames.\n",
    "6. If you are a Python user, use DataFrames and resort back to RDDs if you need more control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
    "\n",
    "Frank Kane's Udemy course: Taming Big Data with Apache Spark and Python - Hands On!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
